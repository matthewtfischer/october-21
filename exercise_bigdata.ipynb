{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Exercises\n",
    "\n",
    "In these exercises we will work on data from a series of global weather monitoring stations used to measure climate trends to examine long-term trends in temperature for your home locality. This data comes from the Global Historical Climatology Network, and is the actual raw data provided by NOAA. The only changes I have made to this data are a few small formatting changes to help meet the learning goals of this exercise. \n",
    "\n",
    "To do these excercises, first please download the data for this exercise [from here](https://www.dropbox.com/s/oq36w90hm9ltgvc/global_climate_data.zip?dl=0). Note this is a big file (this is a big-data exercise, after all), so be patient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** The data we'll be working with can be found in the file `ghcnd_daily.tar.gz`. It includes daily weather data from thousands of weather stations around the work over many decades. \n",
    "\n",
    "Begin by decompressing the file and checking it's size -- it should come out to be *about* 4gb, but will expand to about 12 gb in RAM, which means there's just no way most students (who usually have, at most, 16gb of RAM) can import this dataset into pandas and manipulate it directly. \n",
    "\n",
    "(Unsure how to decompress it? Try checking the readme)\n",
    "\n",
    "(Note: what we're doing can be applied to much bigger datasets, but they sometimes takes hours to work with, so we're working with data that's just a *little* big so we can get exercises done in reasonable time).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Thankfully, we aren't going to be working with *all* the data today. Instead, everyone should pick three weather stations to examine during this analysis. \n",
    "\n",
    "To pick your stations, we'll need to open the `ghcnd-stations.txt` file in the directory you've downloaded. It includes both station codes (which is what we'll find in the `ghcnd_daily.csv` data, as well as the name and location of each station). \n",
    "\n",
    "When picking a weather station, make sure to pick one flagged as being in either GSN, HCN, or CRN (these designate more formalized stations that have been around a long time, ensuring you'll get a station with data that has been recorded over a longer period). \n",
    "\n",
    "Note that Station IDs start with the two-letter code of the country in which they are located, and the \"NAME\" column often constains city names. \n",
    "\n",
    "**The `ghcnd-stations.txt` is a \"fixed-width\" dataset,** meaning that instead of putting commas or tabs between observations, all columns have the same width (in terms of number of characters). So to import this data you'll have to (a) read the notes about the data in the project README.txt, and (b) read about how to read in fixed-width data in pandas. When entering column specifications, remember that normal people count from 1 and include end points, while Python counts from 0 and doesn't include end points (so if the readme says data is in columns 10-20, in Python that'd be 9 through 20)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** Now that we something about the observations we want to work with, we can now turn to our actual weather data. \n",
    "\n",
    "Our daily weather can be found in `ghcnd_daily.csv`, which you get by unzipping `ghcnd_daily.tar.gz`. Note that the README.txt talks about this being a fixed-width file. Since you've already dealt with one fixed-width file, I've just converted this to a CSV, and dropped all the data that isn't \"daily max temperatures\". \n",
    "\n",
    "Let's start with the fun part. **SAVE YOUR NOTEBOOK AND ANY OTHER OPEN FILES!**. Then just try and import the data (`ghcnd_daily.csv`) while watching your Activity Monitor (Mac) or Resource Monitor (Windows) to see what happens.  \n",
    "\n",
    "If you have 8GB of RAM, this should fail miserably. \n",
    "\n",
    "If you have 16GB of RAM, you might just get away with this. But if it *does* load, try sorting the data by year and see how things go. \n",
    "\n",
    "(If you have 32GB of RAM: you're actually probably fine with data this size. Sorry -- datasets big enough to cause big problems for people with 32GB take a long time to chunk on an 8GB computer, and these exercises have to be fast enough to finish in a class period! There are some exercises at the bottom with a REALLY big dataset you can work with.)\n",
    "\n",
    "You may have to kill your kernel, kill VS Code, and start over when this explodes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../global_climate_data/ghcnd_daily.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:921\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:1083\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._convert_column_data\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:1456\u001b[39m, in \u001b[36mpandas._libs.parsers._maybe_upcast\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/numpy/_core/multiarray.py:1148\u001b[39m, in \u001b[36mputmask\u001b[39m\u001b[34m(a, mask, values)\u001b[39m\n\u001b[32m   1100\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1101\u001b[39m \u001b[33;03m    copyto(dst, src, casting='same_kind', where=True)\u001b[39;00m\n\u001b[32m   1102\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1143\u001b[39m \n\u001b[32m   1144\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (dst, src, where)\n\u001b[32m-> \u001b[39m\u001b[32m1148\u001b[39m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath.putmask)\n\u001b[32m   1149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mputmask\u001b[39m(a, /, mask, values):\n\u001b[32m   1150\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1151\u001b[39m \u001b[33;03m    putmask(a, mask, values)\u001b[39;00m\n\u001b[32m   1152\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m \n\u001b[32m   1190\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, mask, values)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../global_climate_data/ghcnd_daily.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)** Now that we know that we can't work with this directly, it's good with these big datasets to just import ~200 lines so you can get a feel for the data. So load *just 200 lines* of `ghcnd_daily.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id  year  month element  value1  mflag1  qflag1 sflag1  value2  \\\n",
      "0    ACW00011604  1949      1    TMAX     289     NaN     NaN      X     289   \n",
      "1    ACW00011604  1949      2    TMAX     267     NaN     NaN      X     278   \n",
      "2    ACW00011604  1949      3    TMAX     272     NaN     NaN      X     289   \n",
      "3    ACW00011604  1949      4    TMAX     278     NaN     NaN      X     283   \n",
      "4    ACW00011604  1949      5    TMAX     283     NaN     NaN      X     283   \n",
      "..           ...   ...    ...     ...     ...     ...     ...    ...     ...   \n",
      "195  AE000041196  1981      9    TMAX   -9999     NaN     NaN    NaN   -9999   \n",
      "196  AE000041196  1981     10    TMAX   -9999     NaN     NaN    NaN     350   \n",
      "197  AE000041196  1981     11    TMAX     330     NaN     NaN      I     310   \n",
      "198  AE000041196  1981     12    TMAX     270     NaN     NaN      I     290   \n",
      "199  AE000041196  1982      1    TMAX     245     NaN     NaN      I     230   \n",
      "\n",
      "     mflag2  ...  qflag29 sflag29  value30  mflag30  qflag30 sflag30  value31  \\\n",
      "0       NaN  ...      NaN       X      272      NaN      NaN       X      272   \n",
      "1       NaN  ...      NaN     NaN    -9999      NaN      NaN     NaN    -9999   \n",
      "2       NaN  ...      NaN       X      278      NaN      NaN       X      267   \n",
      "3       NaN  ...      NaN       X      289      NaN      NaN       X    -9999   \n",
      "4       NaN  ...      NaN       X      294      NaN      NaN       X      300   \n",
      "..      ...  ...      ...     ...      ...      ...      ...     ...      ...   \n",
      "195     NaN  ...      NaN     NaN      390      NaN      NaN       I    -9999   \n",
      "196     NaN  ...      NaN     NaN      340      NaN      NaN       I      330   \n",
      "197     NaN  ...      NaN     NaN      290      NaN      NaN       I    -9999   \n",
      "198     NaN  ...      NaN       I    -9999      NaN      NaN     NaN      290   \n",
      "199     NaN  ...      NaN       I    -9999      NaN      NaN     NaN    -9999   \n",
      "\n",
      "     mflag31  qflag31 sflag31  \n",
      "0        NaN      NaN       X  \n",
      "1        NaN      NaN     NaN  \n",
      "2        NaN      NaN       X  \n",
      "3        NaN      NaN     NaN  \n",
      "4        NaN      NaN       X  \n",
      "..       ...      ...     ...  \n",
      "195      NaN      NaN     NaN  \n",
      "196      NaN      NaN       I  \n",
      "197      NaN      NaN     NaN  \n",
      "198      NaN      NaN       I  \n",
      "199      NaN      NaN     NaN  \n",
      "\n",
      "[200 rows x 128 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../global_climate_data/ghcnd_daily.csv\", nrows=200)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)** Once you have a sense of the data, write code to chunk your data: i.e. code that reads in all blocks of the data that will fit in ram, keeps only the observations for the weather stations you've selected to focus on, and throws away everything else.\n",
    "\n",
    "In addition to your own three weather stations, please also include station USC00050848 (a weather station from near my home!) so you can generate results that we can all compare (to check for accuracy). \n",
    "\n",
    "Note you will probably have to play with your chunk sizes (probably while watching your RAM usage?). That's because small chunk sizes, while useful for debugging, are very slow. \n",
    "\n",
    "Every time Python processes a chunk, there's a fixed processing cost, so in a dataset with, say, 10,000,000 rows, if you try to do chunks of 100 rows, that fixed processing cost has to be paid 100,000 times. Given that, the larger you can make your chunks the better, so long as your chunks don't use up all your RAM. Again, picking a chunk size then watching your RAM usage is a good way to see how close you are to the limits of your RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id  year  month element  value1  mflag1  qflag1 sflag1  value2  \\\n",
      "8    AE000041196  1944      3    TMAX   -9999     NaN     NaN    NaN   -9999   \n",
      "9    AE000041196  1944      4    TMAX     258     NaN     NaN      I     263   \n",
      "10   AE000041196  1944      5    TMAX     335     NaN     NaN      I     363   \n",
      "11   AE000041196  1944      6    TMAX     374     NaN     NaN      I     396   \n",
      "12   AE000041196  1944      7    TMAX     396     NaN     NaN      I     380   \n",
      "..           ...   ...    ...     ...     ...     ...     ...    ...     ...   \n",
      "195  AE000041196  1981      9    TMAX   -9999     NaN     NaN    NaN   -9999   \n",
      "196  AE000041196  1981     10    TMAX   -9999     NaN     NaN    NaN     350   \n",
      "197  AE000041196  1981     11    TMAX     330     NaN     NaN      I     310   \n",
      "198  AE000041196  1981     12    TMAX     270     NaN     NaN      I     290   \n",
      "199  AE000041196  1982      1    TMAX     245     NaN     NaN      I     230   \n",
      "\n",
      "     mflag2  ...  qflag29 sflag29  value30  mflag30  qflag30 sflag30  value31  \\\n",
      "8       NaN  ...      NaN       I      396      NaN      NaN       I      313   \n",
      "9       NaN  ...      NaN       I      346      NaN      NaN       I    -9999   \n",
      "10      NaN  ...      NaN       I      385      NaN      NaN       I      352   \n",
      "11      NaN  ...      NaN       I      435      NaN      NaN       I    -9999   \n",
      "12      NaN  ...      NaN       I      380      NaN      NaN       I      385   \n",
      "..      ...  ...      ...     ...      ...      ...      ...     ...      ...   \n",
      "195     NaN  ...      NaN     NaN      390      NaN      NaN       I    -9999   \n",
      "196     NaN  ...      NaN     NaN      340      NaN      NaN       I      330   \n",
      "197     NaN  ...      NaN     NaN      290      NaN      NaN       I    -9999   \n",
      "198     NaN  ...      NaN       I    -9999      NaN      NaN     NaN      290   \n",
      "199     NaN  ...      NaN       I    -9999      NaN      NaN     NaN    -9999   \n",
      "\n",
      "     mflag31  qflag31 sflag31  \n",
      "8        NaN      NaN       I  \n",
      "9        NaN      NaN     NaN  \n",
      "10       NaN      NaN       I  \n",
      "11       NaN      NaN     NaN  \n",
      "12       NaN      NaN       I  \n",
      "..       ...      ...     ...  \n",
      "195      NaN      NaN     NaN  \n",
      "196      NaN      NaN       I  \n",
      "197      NaN      NaN     NaN  \n",
      "198      NaN      NaN       I  \n",
      "199      NaN      NaN     NaN  \n",
      "\n",
      "[192 rows x 128 columns]\n"
     ]
    }
   ],
   "source": [
    "stations = (\"AE000041196\", \"AG000060590\", \"AG000060611\")\n",
    "chunked_data = data[data[\"id\"].isin(stations)]\n",
    "print(chunked_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(6)** Now, for each weather station, figure out the *earliest* year with data. Keep `USC00050848` and the two of the three weather stations you picked with the best data (i.e., you should have 3 total, two you picked and `USC00050848`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(7)** Now calculate the average max temp for each weather station / month in the data. Note that in a few weeks, we'll have the skills to do this by reshaping our data so each row is a single day, rather than a month. But for the moment, just sum the columns, watching out for weird values. \n",
    "\n",
    "To sum across the value columns, we can combine: \n",
    "\n",
    "```python\n",
    "weather_data.filter(like='value')\n",
    "```\n",
    "\n",
    "(to just get the columns whose names start with \"value\") with `.mean(axis='columns')` (which averages across columns (along rows) rather than the usual averaging across rows (along columns). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(6)** Now for each weather station, generate a separate plot of the daily temperatures over time. You should end up with a plot that looks something like this:\n",
    "\n",
    "![temp_plots_Colorado](images/temp_plots_Boulder_CO.png)\n",
    "\n",
    "**NOTE:** If your plot has little horizontal lines at the tops and bottoms of the temperature plots connecting perfectly vertical temperature lines, it means you made a mistake in how you plotted your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want More Practice?\n",
    "\n",
    "If you *really* want a challenge, the file `ghcnd_daily_30gb.tar.gz` will decompress into `ghcnd_daily.dat`, the full version of the GHCND daily data. It contains not only daily high temps, but also daily low temps, preciptionation, etc. Moreover, it is still in fixed-width format, and is about 30gb in raw form. \n",
    "\n",
    "Importing and chunking this data (with moderate optimizations) took about 2 hours on my computer. \n",
    "\n",
    "If you're up for it, it's a great dataset to wrestling with data in weird formats and chunking. \n",
    "\n",
    "**Pro-tip:** strings take up *way* more space in RAM than numbers, so some columns can be converted to keep the memory footprint of the data down. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
